{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>\n",
    "        SARSA\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    In this notebook we are going to implement a method that learns from experience and uses bootstrapping.\n",
    "    It is known as SARSA because of the elements involved in the update rule:\n",
    "</div>\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{State}_t, \\text{Action}_t, \\text{Reward}_t, \\text{State}_{t+1}, \\text{Action}_{t+1}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    This method follows an on-policy strategy, in which the same policy that is optimized is responsible for scanning the environment.\n",
    "</div>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align:center\">\n",
    "    <b>This notebook belongs to section 5 of the course \"Reinforcement Learning: beginner to master\".</b>\n",
    "    <br><br>\n",
    "    <a href=\"https://www.udemy.com\">Reinforcement Learning: beginner to master</a> (English)\n",
    "    <br>\n",
    "    <a href=\"https://www.udemy.com\">Reinforcement Learning: de principiante a maestro</a> (Spanish)\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr style=\"background-color: transparent\">\n",
    "    <td style=\"width: 45%\">\n",
    "        <a target=\"_parent\" href=\"https://www.evlabs.io\" style=\"float: center\">\n",
    "            <img src=\"img/evlabs-square.png\" width=\"75\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td valign=\"bottom\">\n",
    "        <a target=\"_parent\" href=\"https://www.youtube.com/channel/UCksRNSzWuMV5IfdrPlglqqw\">\n",
    "            <img src=\"img/YouTube.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_parent\" href=\"https://www.linkedin.com/company/evlabs\">\n",
    "            <img src=\"img/LinkedIn.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_parent\" href=\"https://twitter.com/evelabs\">\n",
    "            <img src=\"img/Twitter.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "    <td>\n",
    "        <a target=\"_parent\" href=\"https://github.com/escape-velocity-labs/\">\n",
    "            <img src=\"img/GitHub.png\" width=\"35\"/>\n",
    "        </a> \n",
    "    </td>\n",
    "\n",
    "  </tr>\n",
    "  <tr style=\"background-color: transparent\">\n",
    "    <th style=\"text-align: center; width: 70%\">Escape Velocity Labs</th>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary software libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from envs import Maze\n",
    "from utils import plot_policy, plot_action_values, test_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment, value table and policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the $Q(s, a)$ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_values = np.zeros(shape=(5, 5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the policy $\\pi(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, epsilon=0.):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        av = action_values[state]\n",
    "        return np.random.choice(np.flatnonzero(av == av.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the value table $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_values(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_policy(action_values, env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the algorithm\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    Adapted from Barto & Sutton: \"Reinforcement Learning: An Introduction\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(action_values, policy, episodes, alpha=0.1, gamma=0.99, epsilon=0.2):\n",
    "    \n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = env.reset()\n",
    "        action = policy(state, epsilon)\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_state, epsilon)\n",
    "            \n",
    "            qsa = action_values[state][action]\n",
    "            next_qsa = action_values[next_state][next_action]\n",
    "            action_values[state][action] = qsa + alpha * (reward + gamma * next_qsa - qsa)\n",
    "            state = next_state\n",
    "            action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sarsa(action_values, policy, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show resulting value table $Q(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_action_values(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show resulting policy $\\pi(\\cdot|s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(action_values, env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the resulting agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[1] Reinforcement Learning: An Introduction. Ch. 4: Dynamic Programming](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
