{"cells":[{"cell_type":"markdown","metadata":{"pycharm":{"name":"#%%\n"},"id":"3RumD4Toa8ox"},"source":["# A coding introduction to the Markov Decision Process\n","\n","<br><br>\n","\n","In this notebook we will practice the concepts learned about control tasks and Markov decision processes. In particular, we will get familiar with a software library called OpenAI Gym that will provide us with a simple interface to these tasks.\n","<br><br><br>\n","<div style=\"text-align:center\">\n","    <b>This notebook belongs to section 2 of the course \"Reinforcement Learning: beginner to master\".</b>\n","    <br><br>\n","    <a href=\"https://www.udemy.com\">Reinforcement Learning: beginner to master</a> (English)\n","    <br>\n","    <a href=\"https://www.udemy.com\">Reinforcement Learning: de principiante a maestro</a> (Spanish)\n","</div>\n","\n","<br>\n","\n","<table style=\"width:35%\">\n","  <tr style=\"background-color: transparent\">\n","    <td style=\"width: 45%\">\n","        <a target=\"_parent\" href=\"https://www.evlabs.io\" style=\"float: center\">\n","            <img src=\"img/evlabs-square.png\" width=\"75\"/>\n","        </a>\n","    </td>\n","    <td valign=\"bottom\">\n","        <a target=\"_parent\" href=\"https://www.youtube.com/channel/UCksRNSzWuMV5IfdrPlglqqw\">\n","            <img src=\"img/YouTube.png\" width=\"35\"/>\n","        </a>\n","    </td>\n","    <td>\n","        <a target=\"_parent\" href=\"https://www.linkedin.com/company/evlabs\">\n","            <img src=\"img/LinkedIn.png\" width=\"35\"/>\n","        </a>\n","    </td>\n","    <td>\n","        <a target=\"_parent\" href=\"https://twitter.com/evelabs\">\n","            <img src=\"img/Twitter.png\" width=\"35\"/>\n","        </a>\n","    </td>\n","    <td>\n","        <a target=\"_parent\" href=\"https://github.com/escape-velocity-labs/\">\n","            <img src=\"img/GitHub.png\" width=\"35\"/>\n","        </a>\n","    </td>\n","\n","  </tr>\n","  <tr style=\"background-color: transparent\">\n","    <th style=\"text-align: center; width: 70%\">Escape Velocity Labs</th>\n","  </tr>\n","\n","</table>\n","\n","\n","<br><br>\n"]},{"cell_type":"code","source":["# #installing dependencies\n","# !apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n","# !ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","# !apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n","# !pip -q install gym\n","# !pip -q install pyglet\n","# !pip -q install pyopengl\n","# !pip -q install pyvirtualdisplay\n","# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n","# !pip install gym pyvirtualdisplay > /dev/null 2>&1"],"metadata":{"id":"U4P3ALtLgSeU","executionInfo":{"status":"ok","timestamp":1688956293369,"user_tz":240,"elapsed":4,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","env = gym.make('CartPole-v0')\n","print('observation space:', env.observation_space)\n","print('action space:', env.action_space)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MA3r-SmeYm7W","executionInfo":{"status":"ok","timestamp":1688956293661,"user_tz":240,"elapsed":295,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"bb469d1d-b338-4129-b9b0-25a793eae2d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n","action space: Discrete(2)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["obs = env.reset()\n","env.render()\n","print('initial observation:', obs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEokxl1RYsNQ","executionInfo":{"status":"ok","timestamp":1688956293661,"user_tz":240,"elapsed":10,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"31013138-9316-4f28-ff88-03052d56327a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["initial observation: [ 0.04241393  0.01566367 -0.01349628  0.00875912]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n","If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["!pip install pyvirtualdisplay"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrBowd79Y2_m","executionInfo":{"status":"ok","timestamp":1688956299706,"user_tz":240,"elapsed":6049,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"8230e4a7-aa3f-4791-fb5a-efed29d932c3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay"],"metadata":{"id":"NIjLZShBY5OY","executionInfo":{"status":"ok","timestamp":1688956299706,"user_tz":240,"elapsed":5,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!sudo apt-get install xvfb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oK-pMve1ZygV","executionInfo":{"status":"ok","timestamp":1688956303354,"user_tz":240,"elapsed":3652,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"09148947-0bfc-4f83-d318-96e71bf70e37"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n","0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"]}]},{"cell_type":"code","source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(400, 400))\n","display.start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9QmyFm_Z8pY","executionInfo":{"status":"ok","timestamp":1688956303355,"user_tz":240,"elapsed":25,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"58be5c6e-aaab-4c70-f6a9-cc893cf5b62d"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7fa0a0576890>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["action = env.action_space.sample()\n","obs, r, done, info = env.step(action)\n","print('next observation:', obs)\n","print('reward:', r)\n","print('done:', done)\n","print('info:', info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aujXDT_ogU6N","executionInfo":{"status":"ok","timestamp":1688956303356,"user_tz":240,"elapsed":20,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"6615c0c2-01eb-408b-915a-5d8c3367db4b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["next observation: [ 0.0427272  -0.17926215 -0.0133211   0.29715344]\n","reward: 1.0\n","done: False\n","info: {}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XUCx4ZqeaI1o","executionInfo":{"status":"ok","timestamp":1688956303356,"user_tz":240,"elapsed":17,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"pycharm":{"name":"#%%\n"},"id":"iaavogjLa8o2","executionInfo":{"status":"ok","timestamp":1688956303356,"user_tz":240,"elapsed":16,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["import gym\n","import numpy as np\n","from IPython import display\n","from matplotlib import pyplot as plt\n","import sys,os\n","sys.path.insert(0,'/content')\n","import os,sys\n","from envs import Maze\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"zllPdYz1a8o4"},"source":["## Quick view of the Gym library:\n","<br/>\n","<br/>\n","\n","Gym is a library for reinforcement learning research. It provides us with a simple interface to a large number of tasks, including\n","\n","- Classic control tasks (CartPole, Pendulum, MountainCar, etc)\n","- Classic video games (Space Invaders, Breakout, Pong, etc)\n","- Continuous control tasks\n","- Robotic arm manipulation\n","\n","In this section we are going to get familiar with the five methods that we'll use while solving a control\n","task.\n","\n","\n","<br/>\n","<br/>\n","\n","\n","![title](img/mdp_diagram.svg)\n","\n","###### Source: https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg"]},{"cell_type":"markdown","metadata":{"id":"S8n48pq_a8o4"},"source":["##### Making the environment: Maze()\n","\n","To create an environment, just pass a string with its name to the gym.make method. If the environment exists, the method returns an instance of the gym.Env class, which represents the environment of the task we are going to solve."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"l33Z7I9Oa8o5","executionInfo":{"status":"ok","timestamp":1688956303357,"user_tz":240,"elapsed":17,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()"]},{"cell_type":"markdown","metadata":{"id":"hyl2oVHsa8o5"},"source":["###### env.reset()\n","\n","This method places the environment in its initial state to  and returns it so that the agent can observe it.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gssno8qYa8o5","executionInfo":{"status":"ok","timestamp":1688956303357,"user_tz":240,"elapsed":17,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"1f793760-245f-47fd-ba40-4eacf4c0c83b"},"outputs":[{"output_type":"stream","name":"stdout","text":["The new episode will start in state: (0, 0)\n"]}],"source":["initial_state = env.reset()\n","print(f\"The new episode will start in state: {initial_state}\")"]},{"cell_type":"code","source":["\"\"\"\n","Maze environment used in the first three modules of the course:\n","\"Beginner to Master: Reinforcement Learning\".\n","\"\"\"\n","\n","\n","from typing import Tuple, Dict, Optional, Iterable\n","\n","\n","import gym\n","from gym import spaces\n","import numpy as np\n","\n","\n","class Maze(gym.Env):\n","    \"\"\"\n","    Description:\n","        The environment consists of a grid of (size x size) positions. The agent\n","        starts the episode in location (row=0, col=0) if the environment is instantiated\n","        without exploring starts or from a random location (different from the goal)\n","         if it is. The goal is always at (row=size-1, col=size-1).\n","    Observation:\n","        Type: MultiDiscrete(2)\n","        Num     Observation     Min     Max\n","        0       row coordinate    0       size-1\n","        1       col coordinate    0       size-1\n","    Actions:\n","        Type: Discrete(4)\n","        Num     Action\n","        0       Move up\n","        1       Move right\n","        2       Move down\n","        3       Move left\n","    Reward:\n","        If the environment is instantiated with shaped rewards, then at each time step\n","        the agent will receive a reward signal of the following magnitude:\n","\n","            r = - steps_to_goal(current_state) / steps_to_goal(furthest_state)\n","\n","        This ensures that the maximum reward awarded will be -1.0 and that at the goal\n","        the reward will be 0.\n","\n","        If the environment is instantiated without shaped rewards, then at each time step\n","        the agent will receive a reward of -1.0 until it reaches the goal.\n","    Episode termination:\n","        The episode terminates when the agent reaches the goal state.\n","    \"\"\"\n","\n","    def __init__(self, exploring_starts: bool = False,\n","                 shaped_rewards: bool = False, size: int = 5) -> None:\n","        \"\"\"\n","        Initialize the environment.\n","\n","        Args:\n","            exploring_starts: should the agent restart at a random location or not.\n","            shaped_rewards: should the environment shape the rewards.\n","            size: size of the maze. Will be of shape (size x size).\n","        \"\"\"\n","        super().__init__()\n","        self.exploring_starts = exploring_starts\n","        self.shaped_rewards = shaped_rewards\n","        self.state = (size - 1, size - 1)\n","        self.goal = (size - 1, size - 1)\n","        self.maze = self._create_maze(size=size)\n","        self.distances = self._compute_distances(self.goal, self.maze)\n","        self.action_space = spaces.Discrete(n=4)\n","        self.action_space.action_meanings = {0: 'UP', 1: 'RIGHT', 2: 'DOWN', 3: \"LEFT\"}\n","        self.observation_space = spaces.MultiDiscrete(nvec=[size, size])\n","\n","        self.viewer = None\n","        self.agent_transform = None\n","\n","    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool, Dict]:\n","        \"\"\"\n","        Take an action in the environment and observe the next transition.\n","\n","        Args:\n","            action: An indicator of the action to be taken.\n","\n","        Returns:\n","            The next transition.\n","        \"\"\"\n","        reward = self.compute_reward(self.state, action)\n","        self.state = self._get_next_state(self.state, action)\n","        done = self.state == self.goal\n","        info = {}\n","        return self.state, reward, done, info\n","\n","    def reset(self) -> Tuple[int, int]:\n","        \"\"\"\n","        Reset the environment to execute a new episode.\n","\n","        Returns: State representing the initial position of the agent.\n","        \"\"\"\n","        if self.exploring_starts:\n","            while self.state == self.goal:\n","                self.state = tuple(self.observation_space.sample())\n","        else:\n","            self.state = (0, 0)\n","        return self.state\n","\n","    def render(self, mode: str = 'human') -> Optional[np.ndarray]:\n","        \"\"\"\n","        Render a state of the environment.\n","\n","        Args:\n","            mode: one of 'human' or 'rgb_array'. The first uses pyglet to create a\n","            window in which to display the frames. The second returns the frame\n","            as a (H x W x C) for usage with other visualization libraries.\n","\n","        Returns:\n","            A numpy.ndarray or None.\n","\n","        \"\"\"\n","        assert mode in ['human', 'rgb_array']\n","\n","        screen_size = 600\n","        scale = screen_size / 5\n","\n","        if self.viewer is None:\n","            #from gym.envs.classic_control import rendering\n","            from gym.utils import pyglet_rendering as rendering\n","            # First time the environment is rendered.\n","            self.viewer = rendering.Viewer(screen_size, screen_size)\n","\n","            # Add the background to the viewer.\n","            left, right, top, bottom = 0, screen_size, screen_size, 0\n","            background = rendering.make_polygon([(left, bottom), (left, top),\n","                                                 (right, top), (right, bottom)], filled=True)\n","            background.set_color(.0862, .1411, .2784)\n","            self.viewer.add_geom(background)\n","\n","            for row in range(5):\n","                for col in range(5):\n","\n","                    state = (row, col)\n","                    for next_state in [(row + 1, col), (row - 1, col),\n","                                       (row, col + 1), (row, col - 1)]:\n","                        if next_state not in self.maze[state]:\n","                            # Add the geometry of the edges and walls (i.e. the boundaries between\n","                            # adjacent squares that are not connected).\n","                            row_diff, col_diff = np.subtract(next_state, state)\n","                            left = (col + (col_diff > 0)) * scale - 2 * (col_diff != 0)\n","                            right = ((col + 1) - (col_diff < 0)) * scale + 2 * (col_diff != 0)\n","                            top = (5 - (row + (row_diff > 0))) * scale - 2 * (row_diff != 0)\n","                            bottom = (5 - ((row + 1) - (row_diff < 0))) * scale + 2 * (row_diff != 0)\n","                            wall = rendering.make_polygon(\n","                                [(left, bottom), (left, top), (right, top), (right, bottom)],\n","                                filled=True)\n","                            wall.set_color(1., 1., 1.)\n","                            self.viewer.add_geom(wall)\n","\n","            # Add the geometry of the goal square to the viewer.\n","            left, right, top, bottom = scale * 4 + 10, scale * 5 - 10, scale - 10, 10\n","            goal = rendering.make_polygon([(left, bottom), (left, top),\n","                                           (right, top), (right, bottom)], filled=True)\n","            goal.set_color(.1607, .7803, .6745)\n","\n","            # Add the geometry of the agent to the viewer.\n","            agent = rendering.make_circle(radius=scale * .6 / 2, res=100, filled=True)\n","            self.agent_transform = rendering.Transform()\n","            agent.add_attr(self.agent_transform)\n","            agent.set_color(.894, .247, .3529)\n","            self.viewer.add_geom(agent)\n","\n","            self.viewer.add_geom(goal)\n","\n","        # Update the agent's position in the maze.\n","        agent_col = scale * (self.state[1] + .5)\n","        agent_row = screen_size - scale * (self.state[0] + .5)\n","        self.agent_transform.set_translation(agent_col, agent_row)\n","        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n","\n","    def close(self) -> None:\n","        \"\"\"\n","        Clean up resources before shutting down the environment.\n","\n","        Returns: None.\n","        \"\"\"\n","        if self.viewer:\n","            self.viewer.close()\n","            self.viewer = None\n","\n","    def compute_reward(self, state: Tuple[int, int], action: int) -> float:\n","        \"\"\"\n","        Compute the reward attained by taking action 'a' at state 's'.\n","\n","        Args:\n","            state: the state of the agent prior to taking the action.\n","            action: the action taken by the agent.\n","\n","        Returns:\n","            A float representing the reward signal received by the agent.\n","\n","        \"\"\"\n","        next_state = self._get_next_state(state, action)\n","        if self.shaped_rewards:\n","            return - (self.distances[next_state] / self.distances.max())\n","        return - float(state != self.goal)\n","\n","    def simulate_step(self, state: Tuple[int, int], action: int):\n","        \"\"\"\n","\n","        Simulate (without taking) a step in the environment.\n","\n","        Args:\n","            state: the state of the agent prior to taking the action.\n","            action: the action to simulate the step with.\n","\n","        Returns:\n","            The next transition.\n","\n","        \"\"\"\n","        reward = self.compute_reward(state, action)\n","        next_state = self._get_next_state(state, action)\n","        done = next_state == self.goal\n","        info = {}\n","        return next_state, reward, done, info\n","\n","    def _get_next_state(self, state: Tuple[int, int], action: int) -> Tuple[int, int]:\n","        \"\"\"\n","        Gets the next state after the agent performs action 'a' in state 's'. If there is a\n","        wall in the way, the next state will be the same as the current.\n","\n","        Args:\n","            state: current state (before taking the action).\n","            action: move performed by the agent.\n","\n","        Returns: a State instance representing the new state.\n","        \"\"\"\n","        if action == 0:\n","            next_state = (state[0] - 1, state[1])\n","        elif action == 1:\n","            next_state = (state[0], state[1] + 1)\n","        elif action == 2:\n","            next_state = (state[0] + 1, state[1])\n","        elif action == 3:\n","            next_state = (state[0], state[1] - 1)\n","        else:\n","            raise ValueError(\"Action value not supported:\", action)\n","        if next_state in self.maze[state]:\n","            return next_state\n","        return state\n","\n","    @staticmethod\n","    def _create_maze(size: int) -> Dict[Tuple[int, int], Iterable[Tuple[int, int]]]:\n","        \"\"\"\n","        Creates a representation of the maze as a dictionary where the keys are\n","        the states available to the agent and the values are lists of adjacent\n","        states.\n","\n","        Args:\n","            size: number of elements of each side in the square grid.\n","\n","        Returns: the adjacency list dictionary.\n","        \"\"\"\n","\n","        maze = {(row, col): [(row - 1, col), (row + 1, col), (row, col - 1), (row, col + 1)]\n","                for row in range(size) for col in range(size)}\n","\n","        left_edges = [[(row, 0), (row, -1)] for row in range(size)]\n","        right_edges = [[(row, size - 1), (row, size)] for row in range(size)]\n","        upper_edges = [[(0, col), (-1, col)] for col in range(size)]\n","        lower_edges = [[(size - 1, col), (size, col)] for col in range(size)]\n","        walls = [\n","            [(1, 0), (1, 1)], [(2, 0), (2, 1)], [(3, 0), (3, 1)],\n","            [(1, 1), (1, 2)], [(2, 1), (2, 2)], [(3, 1), (3, 2)],\n","            [(3, 1), (4, 1)], [(0, 2), (1, 2)], [(1, 2), (1, 3)],\n","            [(2, 2), (3, 2)], [(2, 3), (3, 3)], [(2, 4), (3, 4)],\n","            [(4, 2), (4, 3)], [(1, 3), (1, 4)], [(2, 3), (2, 4)],\n","        ]\n","\n","        obstacles = upper_edges + lower_edges + left_edges + right_edges + walls\n","\n","        for src, dst in obstacles:\n","            maze[src].remove(dst)\n","\n","            if dst in maze:\n","                maze[dst].remove(src)\n","\n","        return maze\n","\n","    @staticmethod\n","    def _compute_distances(goal: Tuple[int, int],\n","                           maze: Dict[Tuple[int, int], Iterable[Tuple[int, int]]]) -> np.ndarray:\n","        \"\"\"\n","        Compute the distance to the goal from all other positions in the maze using Dijkstra's\n","        algorithm.\n","\n","        Args:\n","            goal: A tuple representing the location of the goal in a two-dimensional grid.\n","            maze: A dictionary holding the adjacency lists of all locations in the\n","            two-dimensional grid.\n","\n","        Returns: A (H x W) numpy array holding the minimum number of moves for each position\n","        to reach the goal.\n","\n","        \"\"\"\n","        distances = np.full((5, 5), np.inf)\n","        visited = set()\n","        distances[goal] = 0.\n","\n","        while visited != set(maze):\n","            sorted_dst = [(v // 5, v % 5) for v in distances.argsort(axis=None)]\n","            closest = next(x for x in sorted_dst if x not in visited)\n","            visited.add(closest)\n","\n","            for neighbour in maze[closest]:\n","                distances[neighbour] = min(distances[neighbour], distances[closest] + 1)\n","        return distances\n"],"metadata":{"id":"6x8LUv4-ntBn","executionInfo":{"status":"ok","timestamp":1688956303357,"user_tz":240,"elapsed":14,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["!pip uninstall gym -y\n","!pip install gym==0.21.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D1JPXYWhsO-B","executionInfo":{"status":"ok","timestamp":1688957170251,"user_tz":240,"elapsed":2439,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"beaf8312-71b4-4b60-e5e3-6a32d71aaf70"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["Found existing installation: gym 0.25.2\n","Uninstalling gym-0.25.2:\n","  Successfully uninstalled gym-0.25.2\n","Collecting gym==0.21.0\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]}]},{"cell_type":"markdown","metadata":{"id":"n8hm4sCRa8o6"},"source":["##### env.render()\n","\n","This method generates an image that represents the current state of the environment, in the form of a np.ndarray."]},{"cell_type":"code","execution_count":15,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":598},"id":"PgNRmzw6a8o6","executionInfo":{"status":"error","timestamp":1688957170252,"user_tz":240,"elapsed":23,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}},"outputId":"dc31364c-2d00-4bf2-a761-d20652353d2c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  deprecation(\n"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-487a71daece0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"State: {initial_state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n","\u001b[0;32m/content/envs.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m#from gym.envs.classic_control import rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyglet_rendering\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;31m# First time the environment is rendered.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'pyglet_rendering' from 'gym.utils' (/usr/local/lib/python3.10/dist-packages/gym/utils/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["frame = env.render(mode='rgb_array')\n","plt.axis('off')\n","plt.title(f\"State: {initial_state}\")\n","plt.imshow(frame)"]},{"cell_type":"markdown","metadata":{"id":"3CETXWPLa8o7"},"source":["##### env.step()\n","\n","This method applies the action selected by the agent in the environment, to modify it. In response, the environment returns a tuple of four objects:\n","\n","- The next state\n","- The reward obtained\n","- (bool) if the task has been completed\n","- any other relevant information in a python dictionary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tl7vYgwMa8o7","executionInfo":{"status":"aborted","timestamp":1688956303752,"user_tz":240,"elapsed":404,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["action = 2\n","next_state, reward, done, info = env.step(action)\n","print(f\"After moving down 1 row, the agent is in state: {next_state}\")\n","print(f\"After moving down 1 row, we got a reward of: {reward}\")\n","print(\"After moving down 1 row, the task is\", \"\" if done else \"not\", \"finished\")"]},{"cell_type":"markdown","metadata":{"id":"LlCeaPX7a8o8"},"source":["###### Render the new state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGnQ7ynia8o8","executionInfo":{"status":"aborted","timestamp":1688956303752,"user_tz":240,"elapsed":6,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["frame = env.render(mode='rgb_array')\n","plt.axis('off')\n","plt.title(f\"State: {next_state}\")\n","plt.imshow(frame)"]},{"cell_type":"markdown","metadata":{"id":"MYQDHz-xa8o8"},"source":["##### env.close()\n","\n","It completes the task and closes the environment, releasing the associated resources."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGK4Vr2pa8o8","executionInfo":{"status":"aborted","timestamp":1688956303753,"user_tz":240,"elapsed":7,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env.close()"]},{"cell_type":"markdown","metadata":{"id":"VfbUbtnua8o8"},"source":["## Maze environment: Find the exit.\n","\n","\n","In this section we are going to familiarize ourselves with the environment that we'll use in the lessons 2 (dynamic programming), 3 (Monte Carlo methods) and 4 (temporal difference methods). This environment is perfect for learning the basics of Reinforcement Learning because:\n","\n","- It has few states (25)\n","- Transitions between states are deterministic ($p(s', r| s, a) = 1$)\n","- All rewards are the same (-1) until the episode concludes. Thus facilitating the study of the value and action-value functions\n","\n","Through this environment, we are going to review the concepts seen in lesson 1 (The Markov decision process):\n","\n","- States and state space\n","- Actions and action space\n","- Trajectories and episodes\n","- Rewards and returns\n","- Policy\n","\n","\n","The environment is a maze of 5x5 cells, in which the goal of the agent is to find the exit, located in the lower right corner, in the cell (4,4). In the image, the exit is colored in light green.\n","\n","To reach the exit, the agent can take four different actions: move up, move down, move left and move right."]},{"cell_type":"markdown","metadata":{"id":"qF_T6MOca8o9"},"source":["###### Create the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-KxJfZQa8o9","executionInfo":{"status":"aborted","timestamp":1688956303753,"user_tz":240,"elapsed":7,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()"]},{"cell_type":"markdown","metadata":{"id":"CPax2wmZa8o9"},"source":["##### States and state space\n","\n","The states consist of a tuple of two integers, both in the range [0, 4], representing the row and column in which the agent is currently located:\n","<br>\n","<br>\n","\n","\\begin{equation}\n","    s = (row, column) \\;\\\\\n","    row, column \\in \\{0,1,2,3, 4\\}\n","\\end{equation}\n","<br>\n","<br>\n","The state space (set of all possible states in the task) has 25 elements (all possible combinations of rows and columns):\n","\n","\n","\\begin{equation}\n","    Rows \\times Columns \\;\\\\\n","    S = \\{(0, 0), (0, 1), (1, 0), ...\\}\n","\\end{equation}\n","\n","Information about the state space is stored in the env.observation_space property. In this environment, it is of MultiDiscrete([5 5]) type, which means that it consists of two elements (rows and columns), each with 5 different values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gh-q9xcma8o9","executionInfo":{"status":"aborted","timestamp":1688956303754,"user_tz":240,"elapsed":8,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["print(f\"For example, the initial state is: {env.reset()}\")\n","print(f\"The space state is of type: {env.observation_space}\")"]},{"cell_type":"markdown","metadata":{"id":"-1ifh0G3a8o9"},"source":["##### Actions and action space\n","\n","In this environment, there are four different actions and they are represented by integers:\n","\n","\\begin{equation}\n","a \\in \\{0, 1, 2, 3\\}\n","\\end{equation}\n","\n","- 0 -> move up\n","- 1 -> move right\n","- 2 -> move down\n","- 3 -> move left\n","\n","To execute an action, simply pass it as an argument to the env.step method. Information about the action space is stored in the env.action_space property which is of Discrete(4) class. This means that in this case it only consists of an element in the range [0,4), unlike the state space seen above.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g3JKE17Oa8o-","executionInfo":{"status":"aborted","timestamp":1688956303754,"user_tz":240,"elapsed":8,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["print(f\"An example of a valid action is: {env.action_space.sample()}\")\n","print(f\"The action state is of type: {env.action_space}\")"]},{"cell_type":"markdown","metadata":{"id":"BzPj08A0a8o-"},"source":["##### Trajectories and episodes\n","\n","A trajectory is the sequence generated by moving from one state to another (both arbitrary)\n","\n","\\begin{equation}\n","  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\n","\\end{equation}\n","\n","Let's generate a trajectory of 3 moves in code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGSgwXjia8o-","executionInfo":{"status":"aborted","timestamp":1688956303755,"user_tz":240,"elapsed":8,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()\n","state = env.reset()\n","trajectory = []\n","for _ in range(3):\n","    action = env.action_space.sample()\n","    next_state, reward, done, extra_info = env.step(action)\n","    trajectory.append([state, action, reward, done, next_state])\n","    state = next_state\n","env.close()\n","\n","print(f\"Congrats! You just generated your first trajectory:\\n{trajectory}\")"]},{"cell_type":"markdown","metadata":{"id":"rILIrEWGa8o_"},"source":["An episode is a trajectory that goes from the initial state of the process to the final one:\n","\n","\\begin{equation}\n","  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\n","\\end{equation}\n","where T is the terminal state.\n","\n","Let's generate a whole episode in code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1y4AYbFna8o_","executionInfo":{"status":"aborted","timestamp":1688956303756,"user_tz":240,"elapsed":10,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()\n","state = env.reset()\n","episode = []\n","done = False\n","while not done:\n","    action = env.action_space.sample()\n","    next_state, reward, done, extra_info = env.step(action)\n","    episode.append([state, action, reward, done, next_state])\n","    state = next_state\n","env.close()\n","\n","print(f\"Congrats! You just generated your first episode:\\n{episode}\")"]},{"cell_type":"markdown","metadata":{"id":"TeqH1JD7a8o_"},"source":["##### Rewards and returns\n","\n","A reward is numerical feedback that the environment generates when the agent takes an action *a* in a state *s*:\n","\n","\\begin{equation}\n","    r = r(s, a)\n","\\end{equation}\n","\n","Let's generate a reward from the environment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YGJJlV4a8o_","executionInfo":{"status":"aborted","timestamp":1688956303758,"user_tz":240,"elapsed":11,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()\n","state = env.reset()\n","action = env.action_space.sample()\n","_, reward, _, _ = env.step(action)\n","print(f\"We achieved a reward of {reward} by taking action {action} in state {state}\")"]},{"cell_type":"markdown","metadata":{"id":"Wt2-FEpNa8o_"},"source":["The return associated with a moment in time *t* is the sum (discounted) of rewards that the agent obtains from that moment. We are going to calculate $G_0$, that is, the return to the beginning of the episode:\n","\n","\\begin{equation}\n","    G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{T-1} R_T\n","\\end{equation}\n","\n","\n","\n","Let's assume that the discount factor $\\gamma = 0.99$:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEVV30Zha8pA","executionInfo":{"status":"aborted","timestamp":1688956303759,"user_tz":240,"elapsed":12,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()\n","state = env.reset()\n","done = False\n","gamma = 0.99\n","G_0 = 0\n","t = 0\n","while not done:\n","    action = env.action_space.sample()\n","    _, reward, done, _ = env.step(action)\n","    G_0 += gamma ** t * reward\n","    t += 1\n","env.close()\n","\n","print(\n","    f\"\"\"It took us {t} moves to find the exit,\n","    and each reward r(s,a)=-1, so the return amounts to {G_0}\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"LfVQ9arpa8pA"},"source":["##### Policy\n","\n","A policy is a function $\\pi(a|s) \\in [0, 1]$ that gives the probability of an action given the current state. The function takes the state and action as inputs and returns a float in [0,1].\n","\n","Since in practice we will need to compute the probabilities of all actions, we will represent the policy as a function that takes the state as an argument and returns the probabilities associated with each of the actions. Thus, if the probabilities are:\n","\n","[0.5, 0.3, 0.1]\n","\n","we will understand that the action with index 0 has a 50% probability of being chosen, the one with index 1 has 30% and the one with index 2 has 10%.\n","\n","Let's code a policy function that chooses actions randomly:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnQSCRGea8pA","executionInfo":{"status":"aborted","timestamp":1688956303759,"user_tz":240,"elapsed":11,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["def random_policy(state):\n","    return np.array([0.25] * 4)"]},{"cell_type":"markdown","metadata":{"id":"-EvK8C0Sa8pA"},"source":["## Playing an episode with our random policy"]},{"cell_type":"markdown","metadata":{"id":"spXaREiia8pA"},"source":["###### Create and reset the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pA8NNPhWa8pA","executionInfo":{"status":"aborted","timestamp":1688956303759,"user_tz":240,"elapsed":11,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env = Maze()\n","state = env.reset()"]},{"cell_type":"markdown","metadata":{"id":"a2A0E6uCa8pB"},"source":["###### Compute $p(a|s) \\; \\forall a \\in \\{0, 1, 2, 3\\}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NzNKQzsa8pB","executionInfo":{"status":"aborted","timestamp":1688956303760,"user_tz":240,"elapsed":12,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["action_probabilities = random_policy(state)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfbuBqLwa8pB","executionInfo":{"status":"aborted","timestamp":1688956303760,"user_tz":240,"elapsed":12,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["objects = ('Up', 'Right', 'Down', 'Left')\n","y_pos = np.arange(len(objects))\n","\n","plt.bar(y_pos, action_probabilities, alpha=0.5)\n","plt.xticks(y_pos, objects)\n","plt.ylabel('P(a|s)')\n","plt.title('Random Policy')\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rtyzQ3rMa8pB"},"source":["###### Use the policy to play an episode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tv41LOAfa8pF","executionInfo":{"status":"aborted","timestamp":1688956303763,"user_tz":240,"elapsed":15,"user":{"displayName":"Divya Mereddy","userId":"10248213292416480180"}}},"outputs":[],"source":["env.reset()\n","done = False\n","img = plt.imshow(env.render(mode='rgb_array'))\n","while not done:\n","    action = np.random.choice(range(4), 1, p=action_probabilities)\n","    _, _, done, _ = env.step(action)\n","    img.set_data(env.render(mode='rgb_array'))\n","    plt.axis('off')\n","    display.display(plt.gcf())\n","    display.clear_output(wait=True)\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"rUCRaBcWa8pF"},"source":["## Resources"]},{"cell_type":"markdown","metadata":{"id":"FqmJwLr8a8pF"},"source":["[[1] OpenAI gym: classic control environments](https://gym.openai.com/envs/#classic_control)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}