{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJZvnDZ2wFzS/d/jQsKdFy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DWwB7YJnXLI","executionInfo":{"status":"ok","timestamp":1678481704914,"user_tz":360,"elapsed":19997,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"60929acd-572c-406e-c960-9aee0c088b96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'coursera'...\n","remote: Enumerating objects: 694, done.\u001b[K\n","remote: Total 694 (delta 0), reused 0 (delta 0), pack-reused 694\u001b[K\n","Receiving objects: 100% (694/694), 134.71 MiB | 9.31 MiB/s, done.\n","Resolving deltas: 100% (76/76), done.\n","Updating files: 100% (578/578), done.\n"]}],"source":["#for data sets clone above \n","!git clone https://github.com/knazeri/coursera.git"]},{"cell_type":"code","source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","import sys\n","sys.path.append('/content/coursera/deep-learning/1-neural-networks-and-deep-learning/4-building-your-deep-neural-network-step-by-step/')\n","\n","from testCases_v2 import *\n","from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"metadata":{"id":"4Nj230Aqrdow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"metadata":{"id":"NkSRBf9hrf8C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2 - Outline of the Assignment\n","#3 - Initialization\n","#3.1 - 2-layer Neural Network\n","# GRADED FUNCTION: initialize_parameters\n","\n","def initialize_parameters(n_x, n_h, n_y):\n","    \"\"\"\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","    \"\"\"\n","    \n","    np.random.seed(1)\n","    \n","    ### START CODE HERE ### (≈ 4 lines of code)\n","    W1 = np.random.randn(n_h,n_x)*0.01\n","    b1 = np.zeros((n_h,1))\n","    W2 = np.random.randn(n_y,n_h)*0.01\n","    b2 = np.zeros((n_y,1))\n","    ### END CODE HERE ###\n","    \n","    assert(W1.shape == (n_h, n_x))\n","    assert(b1.shape == (n_h, 1))\n","    assert(W2.shape == (n_y, n_h))\n","    assert(b2.shape == (n_y, 1))\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters\n","\n","# GRADED FUNCTION: initialize_parameters_deep\n","\n","def initialize_parameters_deep(layer_dims):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims)            # number of layers in the network\n","\n","    for l in range(1, L):\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        ### END CODE HERE ###\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","        \n","    return parameters"],"metadata":{"id":"SfOg59FZrjGX","executionInfo":{"status":"ok","timestamp":1678576911273,"user_tz":360,"elapsed":3,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["parameters = initialize_parameters(3,2,1)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48zxD_dasRxZ","executionInfo":{"status":"ok","timestamp":1678481705410,"user_tz":360,"elapsed":135,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"290642a9-b6cf-42fd-9d6b-1adb4cedba33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n"," [-0.01072969  0.00865408 -0.02301539]]\n","b1 = [[0.]\n"," [0.]]\n","W2 = [[ 0.01744812 -0.00761207]]\n","b2 = [[0.]]\n"]}]},{"cell_type":"code","source":["# GRADED FUNCTION: linear_forward\n","\n","def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    Z = np.dot(W,A) + b\n","    ### END CODE HERE ###\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache\n"],"metadata":{"id":"BPB_8T1zsXgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"],"metadata":{"id":"lRxENeXZsdKY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678481705531,"user_tz":360,"elapsed":122,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"4b63dcda-bd54-4a34-d861-8357ba097d39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = [[ 3.26295337 -1.23429987]]\n"]}]},{"cell_type":"code","source":["# GRADED FUNCTION: linear_activation_forward\n","\n","def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","        ### END CODE HERE ###\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","        ### END CODE HERE ###\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)             \n","    # linear_cache = (A, W, b)\n","    # activation_cache = (Z)\n","    # cache = ((A, W, b), (Z))\n","    \n","    #print(\"linear_cache = \"+ str(linear_cache))\n","    #print(\"activation_cache = \"+ str(activation_cache))\n","    #print(\"cache = \"+ str(cache))\n","    \n","    return A, cache"],"metadata":{"id":"Eg-HW3u67mJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hqiEyiQ7vLu","executionInfo":{"status":"ok","timestamp":1678481705532,"user_tz":360,"elapsed":8,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"67aeb6ce-7a02-4a3c-ffcd-c7472343411b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["With sigmoid: A = [[0.96890023 0.11013289]]\n","With ReLU: A = [[3.43896131 0.        ]]\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"IpZyLceE-gFw"}},{"cell_type":"code","source":["#d) L-Layer Model\n","# GRADED FUNCTION: L_model_forward\n","\n","def L_model_forward(X, parameters):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    \n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2                  # number of layers in the neural network\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        #print(\"l = \"+str(l))\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        W = parameters['W' + str(l)]\n","        b = parameters['b' + str(l)]\n","        A, cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","        caches.append(cache)\n","        ### END CODE HERE ###\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    #print(\"L = \"+str(L))\n","    W = parameters['W' + str(L)]\n","    b = parameters['b' + str(L)]\n","    AL, cache = linear_activation_forward(A, W, b, activation = \"sigmoid\")\n","    caches.append(cache)\n","    ### END CODE HERE ###\n","    \n","    #print(\"AL_shape = \"+str(AL.shape))\n","    #print(\"X_shape[1] = \"+str(X.shape[1]))\n","    \n","    assert(AL.shape == (1,X.shape[1]))\n","            \n","    return AL, caches\n"],"metadata":{"id":"MPcIqm_I7xDW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, parameters = L_model_forward_test_case_2hidden()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":222},"id":"kdzLuE4-8J99","executionInfo":{"status":"error","timestamp":1678481705642,"user_tz":360,"elapsed":116,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"0c325827-6c12-46b4-cfc2-7afe94688300"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-6129c4a08c70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward_test_case_2hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AL = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of caches list = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'L_model_forward_test_case_2hidden' is not defined"]}]},{"cell_type":"code","source":["#5 - Cost function\n","\n","# GRADED FUNCTION: compute_cost\n","\n","def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    print(\"AL_shape = \"+str(AL.shape))   #(1,3)\n","    print(\"Y_shape = \"+str(Y.shape))     #(1,3)\n","    \n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","    ### START CODE HERE ### (≈ 1 lines of code)\n","    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))   #\n","    ### END CODE HERE ###\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"],"metadata":{"id":"rShlRtxa8MQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#6 - Backward propagation module\n","##6.1 - Linear backward\n","\n","# GRADED FUNCTION: linear_backward\n","\n","def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    dW = (1/m) * np.dot(dZ, A_prev.T)\n","    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    ### END CODE HERE ###\n","    \n","    #print(\"dW_shape\"+str(dW.shape))\n","    #print(\"db_shape\"+str(db.shape))\n","    #print(\"dA_prev_shape\"+str(dA_prev.shape))\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"],"metadata":{"id":"D7mr4rKBT3KF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up some test inputs\n","dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPoki24UUc3X","executionInfo":{"status":"ok","timestamp":1678481945062,"user_tz":360,"elapsed":116,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"da95cbcd-0826-46a6-f6ce-407822a4691e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dA_prev = [[ 0.51822968 -0.19517421]\n"," [-0.40506361  0.15255393]\n"," [ 2.37496825 -0.89445391]]\n","dW = [[-0.10076895  1.40685096  1.64992505]]\n","db = [[0.50629448]]\n"]}]},{"cell_type":"code","source":["#6.2 - Linear-Activation backward\n","def linear_activation_backward(dA, cache, activation):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","    \n","    Arguments:\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    # linear_cache = (A, W, b)\n","    # activation_cache = (Z)\n","    # cache = ((A, W, b), (Z))\n","    \n","    if activation == \"relu\":\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        dZ = relu_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        ### END CODE HERE ###\n","        \n","    elif activation == \"sigmoid\":\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        ### END CODE HERE ###\n","    \n","    return dA_prev, dW, db"],"metadata":{"id":"R-9Tn0axUfAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcwbQh3MUkuh","executionInfo":{"status":"ok","timestamp":1678481976725,"user_tz":360,"elapsed":125,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"9b27fe8c-67d4-4b3d-f56f-5216d2e50189"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"]}]},{"cell_type":"code","source":["#6.3 - L-Model Backward\n","# GRADED FUNCTION: L_model_backward\n","\n","def L_model_backward(AL, Y, caches):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n","    \n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","    \n","    # Initializing the backpropagation\n","    ### START CODE HERE ### (1 line of code)\n","    print(\"L = \"+str(L))\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    #print(\"dAL = \"+str(dAL))\n","    #print(\"#########################\")\n","    ### END CODE HERE ###\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    ### START CODE HERE ### (approx. 2 lines)\n","    current_cache = caches[L-1]\n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n","    print(\"dA\"+ str(L-1)+\" = \"+str(grads[\"dA\" + str(L-1)]))\n","    print(\"dW\"+ str(L)+\" = \"+str(grads[\"dW\" + str(L)]))\n","    print(\"db\"+ str(L)+\" = \"+str(grads[\"db\" + str(L)]))\n","    ### END CODE HERE ###\n","    \n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        ### START CODE HERE ### (approx. 5 lines)\n","        #print(\"############ l = \"+str(l)+\" ############\")\n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","        #print(\"dA\"+ str(l)+\" = \"+str(grads[\"dA\" + str(l)]))\n","        #print(\"dW\"+ str(l + 1)+\" = \"+str(grads[\"dW\" + str(l + 1)]))\n","        #print(\"db\"+ str(l + 1)+\" = \"+str(grads[\"db\" + str(l + 1)]))\n","        #print(\"#########################\")\n","        ### END CODE HERE ###\n","\n","    return grads"],"metadata":{"id":"NH79Rl7MUmwk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print_grads(grads)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"b-YpRNhTUu6g","executionInfo":{"status":"error","timestamp":1678482718652,"user_tz":360,"elapsed":111,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"5d58d018-8883-4c0f-9adf-6fc38da48de7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["L = 2\n","dA1 = [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]]\n","dW2 = [[-0.39202432 -0.13325855 -0.04601089]]\n","db2 = [[0.15187861]]\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-e52805eb4e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_assess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward_test_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_assess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'print_grads' is not defined"]}]},{"cell_type":"code","source":["#6.4 - Update Parameters\n","# GRADED FUNCTION: update_parameters\n","\n","def update_parameters(parameters, grads, learning_rate):\n","    \"\"\"\n","    Update parameters using gradient descent\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n","    ### END CODE HERE ###\n","    return parameters"],"metadata":{"id":"UZorw6geXb5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsSK_O_vXlel","executionInfo":{"status":"ok","timestamp":1678482764874,"user_tz":360,"elapsed":108,"user":{"displayName":"Divya Mereddy","userId":"11416629501838888552"}},"outputId":"abee17d3-726d-4066-b797-8c4d6c6584b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"]}]},{"cell_type":"code","source":["#7 - Conclusion\n"],"metadata":{"id":"k4UXOP7SXnLh"},"execution_count":null,"outputs":[]}]}